{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36159\n",
      "10325\n",
      "10325\n",
      "Corpus length in words: 255726\n",
      "words 12742\n",
      "not frequent 11245\n",
      "remaining 1497\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#metti final data >= 2016\n",
    "\n",
    "data = pd.read_json(\"finalTrumpPreprocessed.json\")\n",
    "print(len(data))\n",
    "data = data[data[\"year\"] >= 2016] \n",
    "print(len(data))\n",
    "data = data[data[\"is_retweet\"] == False] \n",
    "print(len(data))\n",
    "\n",
    "#remove links\n",
    "data[\"modded1\"] = data[\"text\"].str.replace(\"http\\S+\",\" \")\n",
    "\n",
    "data[\"modded1\"] = data[\"modded1\"].str.replace(\"\\n\", \" \")\n",
    "\n",
    "#remove punctuation\n",
    "data['modded1'] = data[\"modded1\"].str.replace(\"[^\\w\\s]\",\" \")\n",
    "data[\"modded1\"] = data[\"modded1\"].str.lower()\n",
    "\n",
    "sentences = data[\"modded1\"].tolist()\n",
    "list_train_set = sentences\n",
    "\n",
    "fullText = \" \".join(sentences)\n",
    "text_in_words = [w for w in fullText.split(' ') if w.strip() != '' or w == '\\n']\n",
    "text_in_words = list(map(lambda s: s.strip(), text_in_words))\n",
    "\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "from collections import Counter\n",
    "word_freq = dict(Counter(fullText.split()))\n",
    "\n",
    "not_freq = [x for x in word_freq if word_freq[x] < 19] \n",
    "\n",
    "shorter = []\n",
    "for sentence in sentences:\n",
    "    t = sentence.split()\n",
    "    q = sentence\n",
    "    for el in t:\n",
    "        if el in not_freq:\n",
    "            q = q.replace(el,\" \")\n",
    "    if q != []:\n",
    "        shorter.append([q])\n",
    "        \n",
    "unique = len(word_freq.keys())-len(not_freq)\n",
    "print(\"words\", len(word_freq.keys()))\n",
    "print(\"not frequent\", len(not_freq))\n",
    "print(\"remaining\", len(word_freq.keys())-len(not_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "vocab = Counter(fullText.split())\n",
    "#print(vocab)\n",
    "\n",
    "corpus = [x.split() for x in sentences]\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2vec_size = 200\n",
    "w2 = Word2Vec(corpus, size=w2vec_size, window=5, min_count=1,sg=1)\n",
    "word_vectors = w2.wv\n",
    "#print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))\n",
    "w2.wv.most_similar(positive=['clinton'])\n",
    "\n",
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils as ku\n",
    "\n",
    "\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "#print(word_index)\n",
    "sequences = [[word_index[t] for t in sent.split()] for sent in sentences[:len(list_train_set)]]\n",
    "test_sequences = [[word_index[t]  for t in sent.split()] for sent in sentences[len(list_train_set):]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10],\n",
       " [9, 10, 15],\n",
       " [9, 10, 15, 11],\n",
       " [9, 10, 15, 11, 168],\n",
       " [9, 10, 15, 11, 168, 390],\n",
       " [9, 10, 15, 11, 168, 390, 19],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596, 4],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596, 4, 37],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596, 4, 37, 424]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1] #i+1\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245404, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(245404, 12743)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = 4\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    #riprova il padding con pre\n",
    "    predictors, lab = input_sequences[:,:-1],input_sequences[:,-1] #tolto -1\n",
    "    label = ku.to_categorical(lab, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n",
    "input_len = max_sequence_len-1\n",
    "\n",
    "#data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "#data\n",
    "print(predictors.shape)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Embedding, Dropout,LSTM,Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12743\n"
     ]
    }
   ],
   "source": [
    "#WEIGHT MATRIX\n",
    "WV_DIM = w2vec_size\n",
    "\n",
    "nb_words = total_words\n",
    "# we initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(total_words, WV_DIM) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "print(nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, w2vec_size, mask_zero=False,weights=[wv_matrix],input_length=input_len,trainable=True)) #mask zero = False\n",
    "model.add(Bidirectional(LSTM(800)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_words, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#con 800 meno di 20 epoch! un po' overfitta, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"postelectionGenerator-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "#history = model.fit(predictors, label, validation_split=0.35, epochs=2, verbose=1)#callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 200)            2548600   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1600)              6406400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12743)             20401543  \n",
      "=================================================================\n",
      "Total params: 29,356,543\n",
      "Trainable params: 29,356,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open('definitiveMaybe.json', 'r') as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "model.load_weights(\"definitiveMaybe13Iters.h5\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    preds = []\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='post') #max sequence len - 1\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        preds.append(predicted)\n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "#    print(preds)\n",
    "    return seed_text.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Socks Ever By The Fake News Media Is Working Overtime Just Reported That Ted Cruz Is Falling In The Polls History All Business Is Just At The Beginning Of The End We Will Probably'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"best socks ever\", 32, model, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: \" laurencristmann sharp_trident\"\n",
      "Laurencristmann Sharp_Trident It It Is So Important To Audit The Federal Reserve And Yet Ted Cruz Missed The Vote On The Bill Clinton Is The Only Person That Can Stop Her In\n",
      "\n",
      "seed: \" today i am greatly\"\n",
      "Today I Am Greatly Honored To Receive The Endorsement Of Me By The 16 500 Border Patrol Agents Was The First Time Ever Make America Great Again Trump2016 Votetrumpsc Makeamericagreatagain Video Trump2016 Newyorkvalues In\n",
      "\n",
      "seed: \" makeamericagreatagain\"\n",
      "Makeamericagreatagain Is The Only Person That Can Stop Her In The Debate Polls Except For Cnn Which I Have The Absolute Right To Pardon Myself But Why Would I Call China\n",
      "\n",
      "seed: \" remember michael\"\n",
      "Remember Michael The Republican Party Is Racking Up Record Amounts Of Small Dollar Donations Fueled By Trump Supporters Nypost Thank You To The Amazing Law Enforcement Officers Americafirst Trump2016 Makeamericagreatagain Omaha Nebraska\n",
      "\n",
      "seed: \" happyindependenceday usa\"\n",
      "Happyindependenceday Usa The U S A G So Bill Is Not Approved Quickly Bailouts For Insurance Companies Like The Other Candidates All Bull Politicians Are All Talk And No Action This Is\n",
      "\n",
      "seed: \" thank you georgia i\"\n",
      "Thank You Georgia I Appreciate All Of The Great State Of Alabama We Need His Vote On Stopping Crime Illegal Immigration Border Wall And An End To Crime Cradling Sanctuary Cities Started The Wall\n",
      "\n",
      "seed: \" crooked hillary said\"\n",
      "Crooked Hillary Said She Was Under Sniper Fire While Surrounded By Usss Turned Out To Be A Total Catastrophe Dhsgov Says It Would Be A Disaster On Jobs And The Economy If We\n",
      "\n",
      "seed: \" just watched former intelligence official\"\n",
      "Just Watched Former Intelligence Official Who Is Pro Life Pro Family Bigleaguetruth Debates2016 Maga Icymi Watch Here Potusinpoland Crookedhillary Is Not Qualified To Be President Because She Suffers From Bad Judgement Hillary Was Set Up\n",
      "\n",
      "seed: \" wow new polls just\"\n",
      "Wow New Polls Just Came Out With Another One Of Their Phony Democrat Votes That Is All Talk And No Action This Is The Single Greatest Witch Hunt In American History Has Done More\n",
      "\n",
      "seed: \" my son erictrump will\"\n",
      "My Son Erictrump Will Be Interviewed On Foxandfriends At 7 30 A M Enjoy The Rose Garden Join Me Live From The Rose Garden Join Me Live From The Rose Garden Join Me Live\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand = sentences\n",
    "sampling = random.choices(rand, k=10)\n",
    "\n",
    "seeds = []\n",
    "for el in sampling:\n",
    "    i = random.randint(2,5)\n",
    "    seeds.append(\" \".join(el.split()[:i]))\n",
    "    \n",
    "for el in seeds:\n",
    "    print(\"seed: \\\" \" + el + \"\\\"\")\n",
    "    t = generate_text(el, 30,model, input_len)\n",
    "    print(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    s = sentence.split()\n",
    "    if \"talk\"  in s and \"no\" in s and \"action\" in s and \"single\" in s:\n",
    "        print(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: \" why is that\"\n",
      "Why Is That Hillary Clintons Family And Dems Dealings With Russia Are Not Looked At But My Non Dealings Are Made Up By Women Many Already Proven False And Pushed\n",
      "\n",
      "seed: \" we are with\"\n",
      "We Are With You And Your Campaign In The History Of Politics B C I Stand 100 Behind Everything We Do What We Do Best We Pull Together We Join\n",
      "\n",
      "seed: \" donald trump is\"\n",
      "Donald Trump Is A Vote To Cut Taxes Big League For The Middle Class And Business Jobs The Reason Obstruction And Delay At This Rate It Would Take 9 Years\n",
      "\n",
      "seed: \" hillary clinton is a bad\"\n",
      "Hillary Clinton Is A Bad Joke And A Hoax Designed To Target Trump Tom Fitton Judicialwatch A Disgrace To Our Country And Our Workers Our Steel Industry Is In Bad Shape If\n",
      "\n",
      "seed: \" great success today at\"\n",
      "Great Success Today At The Whitehouse Congratulations To The Republic Of Korea On Hosting The Olympics What A Wonderful Opportunity To Show Everyone That You Are Totally Anti Trump Little Jeff\n",
      "\n",
      "seed: \" i am hearing so\"\n",
      "I Am Hearing So Badly That It Will Expand In Michigan And Ohio Plants Adding 2000 Jobs This After The Republican Party Is Racking Up Record Amounts Of Small Dollar Donations\n",
      "\n",
      "seed: \" get out and vote\"\n",
      "Get Out And Vote Tomorrow We Will Make America Great Again Trump2016 Votetrumpsc Makeamericagreatagain Video Trump2016 Newyorkvalues In April I Encourage All Americans To Participate In The Pardoning Of The National\n",
      "\n",
      "seed: \" melania and i offer\"\n",
      "Melania And I Offer Our Deepest Condolences To The Family Of Shimon Peres Then After Election Day That Is Why All Of The Great State Of Alabama We Need His Vote\n",
      "\n",
      "seed: \" we must stop being\"\n",
      "We Must Stop Being Politically Correct And Get Down To The Business Of The New York Times And Propaganda Machine For Amazon The Washington Post Is Far More Fiction Than Fact\n",
      "\n"
     ]
    }
   ],
   "source": [
    "goodSeeds = [\"why is that\",\"we are with\",\"donald trump is\",\"hillary clinton is a bad\",\"great success today at\",\"i am hearing so\",\"get out and vote\",\"melania and i offer\",\"we must stop being\"]\n",
    "    \n",
    "for el in goodSeeds:\n",
    "    print(\"seed: \\\" \" + el + \"\\\"\")\n",
    "    t = generate_text(el, 27,model, 3)\n",
    "    print(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why is that hillary clintons family and dems dealings with russia are not looked at  but my non dealings are \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    s = sentence.split()\n",
    "    if \"why\"  in s and \"that\" in s and \"clintons\" and \"family\" in s:\n",
    "        print(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalSeeds = [\"donald trump is\",\"hillary clinton is a bad\",\"great success today at\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
