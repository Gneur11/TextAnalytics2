{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36159\n",
      "10325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['in_reply_to_user_id_str', 'is_retweet', 'text', 'month', 'year',\n",
       "       'hour', 'week_year', 'date', 'modded_text', 'modded1', 'noTags_punct',\n",
       "       'modded1_stemmed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "#metti final data >= 2016\n",
    "\n",
    "data = pd.read_json(\"finalTrumpPreprocessed.json\")\n",
    "print(len(data))\n",
    "data = data[data[\"year\"] >= 2016] \n",
    "print(len(data))\n",
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10325\n"
     ]
    }
   ],
   "source": [
    "data = data[data[\"is_retweet\"] == False] \n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25834    i will be on  foxnews live   with members of m...\n",
       "25835                  happy new year  amp  thank you     \n",
       "25836    happy new year from  maralago  thank you to my...\n",
       "25837      jallenaip  hillary said she was in a  fog of...\n",
       "25838      sprinklermanus   cnn  realdonaldtrump they r...\n",
       "25839    well  the year has officially begun  i have ma...\n",
       "25840      marie7777777777   realdonaldtrump we love u ...\n",
       "25841      jodil792  we are standing with you  spreadin...\n",
       "25842      memeoryhead  i m one of your biggest fans mr...\n",
       "25843      casuperrunner   georgehenryw huckabee is a g...\n",
       "25844    i will be going to mississippi tomorrow night ...\n",
       "25845      codyraymille  i have never been interested i...\n",
       "25846    the person that hillary clinton least wants to...\n",
       "25847    thank you so much to   for naming me the 2015 ...\n",
       "25848     votetrump2016  amp  together we will  makeame...\n",
       "25849    massive crowds expected in mississippi tomorro...\n",
       "25850      jebbush is a sad case  a total embarrassment...\n",
       "25851      jebbush is a low energy  stiff  who should f...\n",
       "25852    hillary clinton said that it is o k  to ban mu...\n",
       "25853    hillary clinton doesn t have the strength or s...\n",
       "25854    i hope bill clinton starts talking about women...\n",
       "25855    when i look at all of the money the special in...\n",
       "25856    remember  i am self funding my campaign  the o...\n",
       "25857    heading to biloxi  mississippi  massive crowds...\n",
       "25858    just arrived in mississippi for the rally  wor...\n",
       "25859      granite_hope   brandonstinney how can you de...\n",
       "25860    thank you  biloxi   mississippi  remember this...\n",
       "25861      namusca   realdonaldtrump  erictrump   diamo...\n",
       "25862      mmmdigits  well done  biloxi    great displa...\n",
       "25863    just returned from mississippi   a great evening \n",
       "                               ...                        \n",
       "36129    so surprised to see conservative thinkers like...\n",
       "36130    as the great people of ia  il   amp  mo contin...\n",
       "36131     they wanted to know what trump was up to with...\n",
       "36132    there is nothing easy about a usa infrastructu...\n",
       "36133                            so great to watch this   \n",
       "36134    when will the radical left wing media apologiz...\n",
       "36135    anything in this very interesting world is pos...\n",
       "36136    very good call yesterday with president putin ...\n",
       "36137    today  may 4th   is international firefighters...\n",
       "36138    how can it be possible that james woods  and m...\n",
       "36139                 mike has been a fantastic senator   \n",
       "36140    the kentuky derby decision was not a good one ...\n",
       "36141    i am pleased to inform all of those that belie...\n",
       "36142    the kentucky derby decision was not a good one...\n",
       "36143    for 10 months  china has been paying tariffs t...\n",
       "36144        of additional goods sent to us by china re...\n",
       "36145        to testify  are they looking for a redo be...\n",
       "36146    after spending more than  35 000 000 over a tw...\n",
       "36147     the report sounded an awful lot as being come...\n",
       "36148     this is not congressional oversight  this is ...\n",
       "36149    despite the tremendous success that i have had...\n",
       "36150        the witch hunt is over but we will never f...\n",
       "36151    pending the confirmation of mark morgan as our...\n",
       "36152        to the gazan people   these terrorist acts...\n",
       "36153    once again  israel faces a barrage of deadly r...\n",
       "36154         the witch hunt is over but we will never ...\n",
       "36155    despite the tremendous success that i have had...\n",
       "36156    also  there are  no high crimes  amp  misdemea...\n",
       "36157     democrat texas congressman al green says impe...\n",
       "36158    the united states has been losing  for many ye...\n",
       "Name: modded1, Length: 10325, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove links\n",
    "data[\"modded1\"] = data[\"text\"].str.replace(\"http\\S+\",\" \")\n",
    "\n",
    "data[\"modded1\"] = data[\"modded1\"].str.replace(\"\\n\", \" \")\n",
    "\n",
    "#remove punctuation\n",
    "data['modded1'] = data[\"modded1\"].str.replace(\"[^\\w\\s]\",\" \")\n",
    "data[\"modded1\"] = data[\"modded1\"].str.lower()\n",
    "data[\"modded1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[\"modded1\"].tolist()\n",
    "list_train_set = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length in words: 255726\n",
      "words 12742\n",
      "not frequent 11245\n",
      "remaining 1497\n"
     ]
    }
   ],
   "source": [
    "fullText = \" \".join(sentences)\n",
    "text_in_words = [w for w in fullText.split(' ') if w.strip() != '' or w == '\\n']\n",
    "text_in_words = list(map(lambda s: s.strip(), text_in_words))\n",
    "\n",
    "print('Corpus length in words:', len(text_in_words))\n",
    "\n",
    "from collections import Counter\n",
    "word_freq = dict(Counter(fullText.split()))\n",
    "\n",
    "not_freq = [x for x in word_freq if word_freq[x] < 19] \n",
    "\n",
    "shorter = []\n",
    "for sentence in sentences:\n",
    "    t = sentence.split()\n",
    "    q = sentence\n",
    "    for el in t:\n",
    "        if el in not_freq:\n",
    "            q = q.replace(el,\" \")\n",
    "    if q != []:\n",
    "        shorter.append([q])\n",
    "        \n",
    "unique = len(word_freq.keys())-len(not_freq)\n",
    "print(\"words\", len(word_freq.keys()))\n",
    "print(\"not frequent\", len(not_freq))\n",
    "print(\"remaining\", len(word_freq.keys())-len(not_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "vocab = Counter(fullText.split())\n",
    "#print(vocab)\n",
    "\n",
    "corpus = [x.split() for x in sentences]\n",
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "w2vec_size = 200\n",
    "w2 = Word2Vec(corpus, size=w2vec_size, window=5, min_count=1,sg=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crooked', 0.9407603740692139),\n",
       " ('dnc', 0.8912924528121948),\n",
       " ('paid', 0.8794649839401245),\n",
       " ('clear', 0.8755836486816406),\n",
       " ('dirty', 0.8678460121154785),\n",
       " ('email', 0.8631898760795593),\n",
       " ('frame', 0.8619242310523987),\n",
       " ('scheme', 0.8608847856521606),\n",
       " ('emails', 0.8606369495391846),\n",
       " ('her', 0.8600423336029053)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = w2.wv\n",
    "#print(\"Number of word vectors: {}\".format(len(word_vectors.vocab)))\n",
    "w2.wv.most_similar(positive=['clinton'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = len(word_vectors.vocab)\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils as ku\n",
    "\n",
    "\n",
    "word_index = {t[0]: i+1 for i,t in enumerate(vocab.most_common(MAX_NB_WORDS))}\n",
    "#print(word_index)\n",
    "sequences = [[word_index[t] for t in sent.split()] for sent in sentences[:len(list_train_set)]]\n",
    "test_sequences = [[word_index[t]  for t in sent.split()] for sent in sentences[len(list_train_set):]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9, 10],\n",
       " [9, 10, 15],\n",
       " [9, 10, 15, 11],\n",
       " [9, 10, 15, 11, 168],\n",
       " [9, 10, 15, 11, 168, 390],\n",
       " [9, 10, 15, 11, 168, 390, 19],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596, 4],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596, 4, 37],\n",
       " [9, 10, 15, 11, 168, 390, 19, 596, 4, 37, 424]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1] #i+1\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245404, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(245404, 12743)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = 4\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    #riprova il padding con pre\n",
    "    predictors, lab = input_sequences[:,:-1],input_sequences[:,-1] #tolto -1\n",
    "    label = ku.to_categorical(lab, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n",
    "input_len = max_sequence_len-1\n",
    "\n",
    "#data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\", truncating=\"post\")\n",
    "#data\n",
    "print(predictors.shape)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 9] [ 0  9 10] [ 9 10 15]\n"
     ]
    }
   ],
   "source": [
    "print(predictors[0],predictors[1],predictors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12743\n"
     ]
    }
   ],
   "source": [
    "#WEIGHT MATRIX\n",
    "WV_DIM = w2vec_size\n",
    "\n",
    "nb_words = total_words\n",
    "# we initialize the matrix with random numbers\n",
    "wv_matrix = (np.random.rand(total_words, WV_DIM) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        wv_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "print(nb_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Embedding, Dropout,LSTM,Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 200)            2548600   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1600)              6406400   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12743)             20401543  \n",
      "=================================================================\n",
      "Total params: 29,356,543\n",
      "Trainable params: 29,356,543\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, w2vec_size, mask_zero=False,weights=[wv_matrix],input_length=input_len,trainable=True)) #mask zero = False\n",
    "model.add(Bidirectional(LSTM(800)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_words, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#con 800 meno di 20 epoch! un po' overfitta, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_json = model.to_json()\n",
    "#with open(\"definitiveMaybe.json\", \"w\") as json_file:\n",
    "#    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights('definitiveMaybe15ItersMaybeTooOverfitting.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"postelectionGenerator-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 159512 samples, validate on 85892 samples\n",
      "Epoch 1/2\n",
      "159512/159512 [==============================] - 1519s 10ms/step - loss: 1.4213 - acc: 0.6797 - val_loss: 7.3288 - val_acc: 0.1690\n",
      "Epoch 2/2\n",
      "159512/159512 [==============================] - 1518s 10ms/step - loss: 1.3754 - acc: 0.6918 - val_loss: 7.4454 - val_acc: 0.1697\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, validation_split=0.35, epochs=2, verbose=1)#callbacks=callbacks_list)\n",
    "#5 + 5 + 3 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(predictors, label, validation_split=0.40, epochs=25, verbose=1,callbacks=callbacks_list)\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.load_weights(\"postelectionGenerator-12-0.16.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    preds = []\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0] \n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='post') #max sequence len - 1\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        preds.append(predicted)\n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "#    print(preds)\n",
    "    return seed_text.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#load another model\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open('veryGoodAndBigOne.json', 'r') as json_file:\n",
    "    model = model_from_json(json_file.read())\n",
    "model.load_weights(\"veryGoodAndBigOne.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Socks Ever There They Are No Longer A Credible Source Of The Many Wonderful Things That He Stood For Honor Him For Being The Final Two And Heading Into A September Runoff In Alabama'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"best socks ever\", 32, model, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: \" the local politicians\"\n",
      "The Local Politicians And Build The Wall Democrats Are Protecting Ms 13 Thugs Being Hit On The No Russian Collusion In The History Of The Fbi And\n",
      "\n",
      "seed: \" i wish people\"\n",
      "I Wish People Are The Real Deal And Those That Want To Make A Deal With Iran 1 In Terror No Problem Is Doing So Badly Compared\n",
      "\n",
      "seed: \" i asked vp\"\n",
      "I Asked Vp Pence To Leave Stadium If Any Players Kneeled Disrespecting Our Country Or Our Flag They Said It Loud And Clear She Spent Big Money\n",
      "\n",
      "seed: \" a democratic lawmaker\"\n",
      "A Democratic Lawmaker Just Introduced A Bill To Repeal Obamacare And Give Americans Many Choices And Much Lower Prices At The Pharmacy Counter Intelligence Operation Into The\n",
      "\n",
      "seed: \" total inaction on\"\n",
      "Total Inaction On Daca By Dems Where Are You A Deal Can Be Made Without Causing Disruption The Focus Must Be Life And Safety Daca Which Is\n",
      "\n",
      "seed: \" sc has kept\"\n",
      "Sc Has Kept Us Safe From Exec Amnesty For Now But They Are Fading Fast And The Dems Remember The Dirty Dossier Uranium To The Fbi And\n",
      "\n",
      "seed: \" i have brought\"\n",
      "I Have Brought Millions Of People Who Voted To Make America Great Again Americafirst Imwithyou Maga Tickets Available At The Nytimes For The Lame Hit Piece They\n",
      "\n",
      "seed: \" china has a\"\n",
      "China Has A Business Tax Rate Of 15 We Should Do Everything Possible To Assist Amp Support The People Of Iowa Find Your Iacaucus Location At So\n",
      "\n",
      "seed: \" if the fed\"\n",
      "If The Fed Said Yesterday That Unemployment Could Drop Again Foxandfriends Kilmeade To Vote Trump In The News That They Would Not Watch The Debate Tonight Will\n",
      "\n",
      "seed: \" best january for\"\n",
      "Best January For The First Time In Many Decades Very Exciting Times For Our Country I Will Solve And Fast The Word Must Finally Be Getting Out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand = sentences\n",
    "sampling = random.choices(rand, k=10)\n",
    "\n",
    "seeds = []\n",
    "for el in sampling:\n",
    "    seeds.append(\" \".join(el.split()[:3]))\n",
    "    \n",
    "for el in seeds:\n",
    "    print(\"seed: \\\" \" + el + \"\\\"\")\n",
    "    t = generate_text(el, 24,model, input_len)\n",
    "    print(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i asked  vp pence to leave stadium if any players kneeled  disrespecting our country  i am proud of him and  secondlady karen \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    s = sentence.split()\n",
    "    if \"vp\"  in s and \"kneeled\" in s and \"players\" in s:\n",
    "        print(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"Citterio is buonista and his communist\", 5 , model, input_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodSeeds = [\"why is that\",\"we are with\",\"donald trump is\",\"hillary clinton is a bad\",\"great success today at\",\"i am hearing so\",\"get out and vote\",\"melania and i offer\",\"we must stop being\"]\n",
    "    \n",
    "for el in goodSeeds:\n",
    "    print(\"seed: \\\" \" + el + \"\\\"\")\n",
    "    t = generate_text(el, 24,model, max_sequence_len)\n",
    "    print(t + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    s = sentence.split()\n",
    "    if \"network\"  in s and \"news\" in s and \"become\" in s :\n",
    "        print(sentence + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
